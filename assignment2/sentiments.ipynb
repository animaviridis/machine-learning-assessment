{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('assignment2')\n",
    "\n",
    "import aux_functions as aux\n",
    "import bayes\n",
    "import rule_based\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialise datasets and dictionaries\n",
    "sentimentDictionary, sentencesTrain, sentencesTest, sentencesNokia = aux.read_files()\n",
    "\n",
    "# build conditional probabilities using training data\n",
    "pWordPos, pWordNeg, pWord = bayes.trainBayes(sentencesTrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "run naive bayes classifier on datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Films (Train Data, Naive Bayes)\t Accuracy (All)=0.93 (8956/9640)\n\nFilms (Train Data, Naive Bayes)\t Precision (Pos)=0.93 (4433/4743)\nFilms (Train Data, Naive Bayes)\t Recall (Pos)=0.92 (4433/4807)\nFilms (Train Data, Naive Bayes)\t F-measure (Pos)=0.93\nFilms (Train Data, Naive Bayes)\t Precision (Neg)=0.92 (4523/4897)\nFilms (Train Data, Naive Bayes)\t Recall (Neg)=0.94 (4523/4833)\nFilms (Train Data, Naive Bayes)\t F-measure (Neg)=0.93\n\n"
     ]
    }
   ],
   "source": [
    "# run naive bayes classifier on datasets\n",
    "bayes.testBayes(sentencesTrain,  \"Films (Train Data, Naive Bayes)\\t\", pWordPos, pWordNeg, pWord,0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Films  (Test Data, Naive Bayes)\t Accuracy (All)=0.79 (857/1078)\n\nFilms  (Test Data, Naive Bayes)\t Precision (Pos)=0.81 (398/489)\nFilms  (Test Data, Naive Bayes)\t Recall (Pos)=0.75 (398/528)\nFilms  (Test Data, Naive Bayes)\t F-measure (Pos)=0.78\nFilms  (Test Data, Naive Bayes)\t Precision (Neg)=0.78 (459/589)\nFilms  (Test Data, Naive Bayes)\t Recall (Neg)=0.83 (459/550)\nFilms  (Test Data, Naive Bayes)\t F-measure (Neg)=0.81\n\n"
     ]
    }
   ],
   "source": [
    "bayes.testBayes(sentencesTest,  \"Films  (Test Data, Naive Bayes)\\t\", pWordPos, pWordNeg, pWord,0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nokia   (All Data,  Naive Bayes)\t Accuracy (All)=0.59 (156/266)\n\nNokia   (All Data,  Naive Bayes)\t Precision (Pos)=0.81 (100/124)\nNokia   (All Data,  Naive Bayes)\t Recall (Pos)=0.54 (100/186)\nNokia   (All Data,  Naive Bayes)\t F-measure (Pos)=0.65\nNokia   (All Data,  Naive Bayes)\t Precision (Neg)=0.39 (56/142)\nNokia   (All Data,  Naive Bayes)\t Recall (Neg)=0.70 (56/80)\nNokia   (All Data,  Naive Bayes)\t F-measure (Neg)=0.50\n\n"
     ]
    }
   ],
   "source": [
    "bayes.testBayes(sentencesNokia, \"Nokia   (All Data,  Naive Bayes)\\t\", pWordPos, pWordNeg, pWord,0.7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Films (Train Data, Rule-Based)\t Accuracy (All)=0.50 (4835/9640)\n\nFilms (Train Data, Rule-Based)\t Precision (Pos)=0.50 (4797/9592)\nFilms (Train Data, Rule-Based)\t Recall (Pos)=1.00 (4797/4807)\nFilms (Train Data, Rule-Based)\t F-measure (Pos)=0.67\nFilms (Train Data, Rule-Based)\t Precision (Neg)=0.79 (38/48)\nFilms (Train Data, Rule-Based)\t Recall (Neg)=0.01 (38/4833)\nFilms (Train Data, Rule-Based)\t F-measure (Neg)=0.02\n\n"
     ]
    }
   ],
   "source": [
    "# run sentiment dictionary based classifier on datasets\n",
    "rule_based.testDictionary(sentencesTrain,  \"Films (Train Data, Rule-Based)\\t\", sentimentDictionary, -4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Films  (Test Data, Rule-Based)\t Accuracy (All)=0.50 (534/1078)\n\nFilms  (Test Data, Rule-Based)\t Precision (Pos)=0.49 (528/1072)\nFilms  (Test Data, Rule-Based)\t Recall (Pos)=1.00 (528/528)\nFilms  (Test Data, Rule-Based)\t F-measure (Pos)=0.66\nFilms  (Test Data, Rule-Based)\t Precision (Neg)=1.00 (6/6)\nFilms  (Test Data, Rule-Based)\t Recall (Neg)=0.01 (6/550)\nFilms  (Test Data, Rule-Based)\t F-measure (Neg)=0.02\n\n"
     ]
    }
   ],
   "source": [
    "rule_based.testDictionary(sentencesTest,  \"Films  (Test Data, Rule-Based)\\t\",  sentimentDictionary, -4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nokia   (All Data, Rule-Based)\t Accuracy (All)=0.70 (186/266)\n\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "float division by zero",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-219c1c931565>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrule_based\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtestDictionary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentencesNokia\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Nokia   (All Data, Rule-Based)\\t\"\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0msentimentDictionary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32mC:\\Users\\anima\\Documents\\Aberdeen\\MSc_AI\\CS5062_Machine_Learning\\assessment\\code\\ml_assessment\\assignment2\\rule_based.py\u001b[0m in \u001b[0;36mtestDictionary\u001b[0;34m(sentences_test, data_name, sentiment_dictionary, threshold)\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0mprecision_pos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcorrectpos\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotalpospred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0mrecall_pos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcorrectpos\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotalpos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m     \u001b[0mprecision_neg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcorrectneg\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotalnegpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m     \u001b[0mrecall_neg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcorrectneg\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotalneg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mf_pos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mprecision_pos\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mrecall_pos\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mprecision_pos\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mrecall_pos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: float division by zero"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "rule_based.testDictionary(sentencesNokia, \"Nokia   (All Data, Rule-Based)\\t\",  sentimentDictionary, -3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NEGATIVE:\n['mediocre', 'badly', 'generic', 'routine', 'unfunny', 'waste', 'lame', 'ends_up', 'the_problem', 'car', 'dull', 'supposed_to', 'poorly', 'boring', 'mindless', 'stale', 'only_thing', 'pointless', 'bore', 'not_enough', 'annoying', 'disguise', '90_minutes', 'offensive', 'shoot', 'tiresome', 'waste_of', 'it_were', 'sit_through', 'product', \"there's_not\", 'numbers', 'a_script', 'unless', 'animal', 'horrible', 'harvard', 'far_too', 'wasted', 'save', 'bad_movie', 'plodding', 'lousy', 'inept', 'lifeless', 'not_much', 'pinocchio', 'the_worst', 'unfortunately', 'the_numbers']\n\nPOSITIVE:\n['tour', 'and_how', 'subversive', 'has_created', 'created_a', 'a_delightful', 'your_heart', 'offers_a', 'martha', 'a_compelling', 'detailed', 'resonant', 'best_films', 'and_moving', 'charming_and', 'provides', 'captures', 'intense', 'delight', 'lively', 'spare', 'a_terrific', 'respect', 'tender', 'wry', 'heartwarming', 'wonderfully', 'an_enjoyable', 'a_sweet', 'beauty', 'vividly', 'is_at', 'a_smart', 'wonderful', 'an_engaging', 'gem', 'mesmerizing', 'love_and', 'chilling', 'in_years', 'refreshingly', 'realistic', 'performances_from', 'riveting', 'refreshing', 'what_makes', 'inventive', 'unique', 'engrossing', 'warm']\n"
     ]
    }
   ],
   "source": [
    "# print most useful words\n",
    "aux.mostUseful(pWordPos, pWordNeg, pWord, 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
